{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h-fG8RVc5oZG",
        "outputId": "5f565c08-650b-4e39-9070-0199df328224"
      },
      "outputs": [],
      "source": [
        "!chmod +x setup_env.sh\n",
        "!./setup_env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SDQjQzKlX2mY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/NeMo/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-06-27 00:51:43 nemo_logging:349] /home/tejanikhil/miniconda3/envs/IASNLP/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:220: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, gradient_accumulation_fusion,\n",
            "    \n",
            "[NeMo W 2025-06-27 00:51:43 nemo_logging:349] /home/tejanikhil/miniconda3/envs/IASNLP/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:250: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import nemo.collections.asr as nemo_asr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from utils import load_waveform, extract_prosodic_feature, pad_tensor, custom_audio_collate_fn\n",
        "from Dataset import AudioDataset\n",
        "from config import CONFIG\n",
        "from torch.utils.data import DataLoader\n",
        "from model import ClassificationHead, StressClassifier\n",
        "from train_test import train, test\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger('nemo_logger').setLevel(logging.ERROR)\n",
        "logging.getLogger('nemo').setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make the dataset Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keep the `Denoise_train.rar`, `label.csv` file in the current path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <b>Warning:</b> Run only when you are running it for the first time\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrUZNB7W8ZSX",
        "outputId": "0e014a70-c4bd-4c96-fec8-76a8254a6b05"
      },
      "outputs": [],
      "source": [
        "# !unrar x ./Dataset/Denoise_train.rar ./Dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-SFli7Ug8Sl_"
      },
      "outputs": [],
      "source": [
        "# !mkdir ./Dataset/input_ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the dataset compatable with the nemo preprocessor and keep it in the folder named `/Dataset/input_ready/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "collapsed": true,
        "id": "GbzF4OV88PXB",
        "outputId": "e5028717-b188-4cab-d41f-b7391f2a7f15"
      },
      "outputs": [],
      "source": [
        "# Google Drive\n",
        "Raw_DatasetPath = CONFIG[\"raw_audio_path\"]\n",
        "InputReady_DatasetPath = \"./Dataset/input_ready/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "    <b>Warning:</b> Run only when you are running it for the first time\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# files = os.listdir(Raw_DatasetPath)\n",
        "\n",
        "# for file in files[:10]:\n",
        "#     input_path = os.path.join(Raw_DatasetPath, file)\n",
        "#     output_path = os.path.join(InputReady_DatasetPath, file)\n",
        "#     !ffmpeg -i \"{input_path}\" -ac 1 -ar 16000 \"{output_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the nemo model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "32d3816c99ed4b978b7e73baa3cd675a",
            "73bf45e6cf8f433d8785c592e797d516",
            "67ee82415fe74cf58b03a03338f1166e",
            "b02b95c3f29443bea90128f05d48e892",
            "dfbe1efebd354ce888b502b226937943",
            "8b519c723d154795b250a8f449061e9b",
            "ac573c10703c494a8bda16ab1db72bd1",
            "c30d5953402a42de92142736ea45ad11",
            "e4e44a41393a45d99af8067d2ff29370",
            "f1b84c7bdc4c4963b7931692abbbce73",
            "6594375ac76147909b95f84a38493721"
          ]
        },
        "collapsed": true,
        "id": "W1qZxWUt8QUW",
        "outputId": "3b73ed55-29ae-45f1-bae3-913249a23f48"
      },
      "outputs": [],
      "source": [
        "# model = nemo_asr.models.ASRModel.from_pretrained(\"ai4bharat/indicconformer_stt_hi_hybrid_rnnt_large\")\n",
        "model = nemo_asr.models.ASRModel.restore_from(\"./trained_model/nemo_conformer.nemo\")\n",
        "encoder = model.encoder\n",
        "preprocessor =model.preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30d4167f",
        "outputId": "73d7a36b-73d6-488d-8269-7dd9b3385ed6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AudDataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInputReady_DatasetPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_csv_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m AudDataLoader \u001b[38;5;241m=\u001b[39m DataLoader(AudDataset, batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], collate_fn\u001b[38;5;241m=\u001b[39mcustom_audio_collate_fn, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/Desktop/GitRepositories/IASNLP-2025/Dataset.py:34\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[0;34m(self, audio_paths, csv_path, preprocessor, device, max_audio_sequence_length, max_token_seq_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdenoised_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio Link\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(audio_paths):\n\u001b[0;32m---> 34\u001b[0m     mfcc_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase1_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/denoised_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAudio Link\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mappend(mfcc_feat[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_frames\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(mfcc_feat[\u001b[38;5;241m1\u001b[39m]))\n",
            "File \u001b[0;32m~/Desktop/GitRepositories/IASNLP-2025/Dataset.py:88\u001b[0m, in \u001b[0;36mAudioDataset.phase1_preprocessing\u001b[0;34m(self, audio)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mphase1_preprocessing\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio):\n\u001b[1;32m     87\u001b[0m   features, features_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_waveform(audio)\n\u001b[0;32m---> 88\u001b[0m   batch_size, feature_size, valid_time_frames \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     89\u001b[0m   padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_tensor(features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_audio_sequence_length \u001b[38;5;241m-\u001b[39m valid_time_frames)\n\u001b[1;32m     90\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [padded, valid_time_frames]\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "AudDataset = AudioDataset(audio_paths=InputReady_DatasetPath, csv_path=CONFIG[\"train_csv_path\"], preprocessor=preprocessor, device=device)\n",
        "AudDataLoader = DataLoader(AudDataset, batch_size=CONFIG[\"batch_size\"], collate_fn=custom_audio_collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fix the `encoder_output_shape` and `prosody_features_shape` by manually passing a `.wav` file to the encoder and opensmile. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTfoRwTBf8sX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output shape :  (1325, 80)\n",
            "Prosody features shape:  (256, 7)\n"
          ]
        }
      ],
      "source": [
        "f, f_len = load_waveform(\"./Dataset/input_ready/denoised_ISLE_SESS0011_BLOCKD01_46_sprt1.wav\", preprocessor=preprocessor, max_audio_sequence_length=CONFIG[\"max_audio_sequence_length\"], device=model.device)\n",
        "encoder_output_shape = (f.shape[0],f.shape[1])\n",
        "print(\"Encoder output shape : \", encoder_output_shape)\n",
        "f_pros = extract_prosodic_feature(\"./Dataset/input_ready/denoised_ISLE_SESS0011_BLOCKD01_46_sprt1.wav\", 256)\n",
        "prosody_shape = (f_pros.shape[0],f_pros.shape[1])\n",
        "print(\"Prosody features shape: \", prosody_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Freeze` the encoder parameters to ensure encoder is not getting trained during finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xohruw90RYHA"
      },
      "outputs": [],
      "source": [
        "encoder.freeze()\n",
        "encoder = encoder.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDm2Dnpamro7"
      },
      "outputs": [],
      "source": [
        "classifier_head = ClassificationHead(encoder_output_shape=encoder_output_shape,\n",
        "                                     prosody_shape=prosody_shape,\n",
        "                                     max_output_seq_length=CONFIG[\"max_output_token_length\"],\n",
        "                                     word_level_feature_dim=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TKUvZi3cAIc"
      },
      "outputs": [],
      "source": [
        "epochs = CONFIG[\"epochs\"]\n",
        "batch_size = CONFIG[\"batch_size\"]\n",
        "learning_rate = CONFIG[\"lr\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Patch the encoder and CustomModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88k1I18Jb9PD"
      },
      "outputs": [],
      "source": [
        "CustomModel = StressClassifier(encoder=encoder, classifier_head=classifier_head).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(CustomModel.classifier_head.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([444, 888]), torch.Size([252, 1080]), torch.Size([731, 601]), torch.Size([316, 1016]), torch.Size([380, 952]), torch.Size([316, 1016]), torch.Size([303, 1029]), torch.Size([514, 818])]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [444, 888] at entry 0 and [252, 1080] at entry 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCustomModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAudDataLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/GitRepositories/IASNLP-2025/train_test.py:13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch_audio, valid_frames, batch_prosody, batch_labels) \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     14\u001b[0m     audio_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch_audio, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     valid_frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(valid_frames)\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/miniconda3/envs/IASNLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
            "File \u001b[0;32m~/miniconda3/envs/IASNLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/IASNLP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/GitRepositories/IASNLP-2025/utils.py:60\u001b[0m, in \u001b[0;36mcustom_audio_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m collated_input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(input_features_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     59\u001b[0m collated_time_frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(valid_time_frames_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m collated_prosodic_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprosodic_features_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m collated_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(labels_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collated_input_features, collated_time_frames, collated_prosodic_features, collated_labels\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [444, 888] at entry 0 and [252, 1080] at entry 1"
          ]
        }
      ],
      "source": [
        "train(CustomModel, AudDataLoader, optimizer, criterion, device, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_as = CONFIG[\"model_save_path\"] + f\"/Pretrained_Saved({datetime.date.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\"\n",
        "torch.save(CustomModel.state_dict(), save_as)\n",
        "print(f\"Model saved to {save_as}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded = StressClassifier(encoder=encoder, classifier_head=classifier_head).to(device)\n",
        "model_loaded.load_state_dict(torch.load(save_as))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing on train data\n",
        "df = pd.read_excel(CONFIG[\"train_csv_path\"])\n",
        "df = df.drop(\"Transcript\", axis=1)\n",
        "merged_df = df.copy()\n",
        "merged_df['Label'] = df.iloc[:, 1:].values.tolist()\n",
        "merged_df['Label'] = merged_df['Label'].apply(lambda x: [i for i in x if pd.notna(i)])\n",
        "# Keep only 'Audio Link' and the new merged column\n",
        "\n",
        "training_labels = merged_df[['Audio Link', 'Label']].to_dict(orient=\"records\")\n",
        "test_acc_on_training_data = []\n",
        "for i in res:\n",
        "    if \"denoised_\"+i[\"Audio Link\"]+\".wav\" in os.listdir(\"/content/input_ready/\"):\n",
        "        audio_file_name = audio_paths + \"/denoised_\" + i[\"Audio Link\"] + \".wav\" \n",
        "        preds = test(audio_file_name, model_loaded, encoder.preprocessor, device)\n",
        "        expected = pad_tensor(i[\"Label\"], int(CONFIG[\"max_output_length\"]) - len(i[\"Label\"]))\n",
        "        crct_pred += (preds == expected.bool()).sum().item()\n",
        "        actual_pred += labels.numel()\n",
        "        test_acc_on_training_data.append(100 * crct_preds/actual_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n",
            "torch.Size([1352, 80]) torch.Size([256, 7])\n"
          ]
        }
      ],
      "source": [
        "from utils import extract_prosodic_feature, load_waveform\n",
        "for file in os.listdir(\"./Dataset/input_ready\"):\n",
        "    pf = load_waveform(\"./Dataset/input_ready/\"+file, preprocessor, 1352, device)[0].shape\n",
        "    pros = extract_prosodic_feature(\"./Dataset/input_ready/\"+file, 256)\n",
        "    print(pf, pros.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IASNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32d3816c99ed4b978b7e73baa3cd675a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73bf45e6cf8f433d8785c592e797d516",
              "IPY_MODEL_67ee82415fe74cf58b03a03338f1166e",
              "IPY_MODEL_b02b95c3f29443bea90128f05d48e892"
            ],
            "layout": "IPY_MODEL_dfbe1efebd354ce888b502b226937943"
          }
        },
        "6594375ac76147909b95f84a38493721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67ee82415fe74cf58b03a03338f1166e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c30d5953402a42de92142736ea45ad11",
            "max": 523192320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4e44a41393a45d99af8067d2ff29370",
            "value": 523192320
          }
        },
        "73bf45e6cf8f433d8785c592e797d516": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b519c723d154795b250a8f449061e9b",
            "placeholder": "​",
            "style": "IPY_MODEL_ac573c10703c494a8bda16ab1db72bd1",
            "value": "(…)cconformer_stt_hi_hybrid_rnnt_large.nemo: 100%"
          }
        },
        "8b519c723d154795b250a8f449061e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac573c10703c494a8bda16ab1db72bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b02b95c3f29443bea90128f05d48e892": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1b84c7bdc4c4963b7931692abbbce73",
            "placeholder": "​",
            "style": "IPY_MODEL_6594375ac76147909b95f84a38493721",
            "value": " 523M/523M [00:05&lt;00:00, 48.8MB/s]"
          }
        },
        "c30d5953402a42de92142736ea45ad11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfbe1efebd354ce888b502b226937943": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e44a41393a45d99af8067d2ff29370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1b84c7bdc4c4963b7931692abbbce73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
